# Pixelle MCP Project Configuration File Example
# 
# Configuration loading logic:
# 1. When each service starts, it first loads the config.yml in its service directory (e.g., mcp-base/config.yml)
# 2. If local configuration doesn't exist, it falls back to loading config.yml from the project root directory
# 3. Both configurations won't be loaded simultaneously, only the first found configuration file will be loaded
# 4. After loading, the corresponding service block configuration will be automatically injected into environment variables (variable names in uppercase)
# 
# Usage recommendations:
# - Global deployment: Copy this file to the root directory as config.yml, all services share one configuration file
# - Separate deployment: Copy the corresponding blocks to config.yml in each service directory for independent configuration


# Base service configuration
base:
  # Timezone setting
  tz: Asia/Shanghai
  
  # Service configuration
  server_host: localhost
  server_port: 9001
  # Optional, used to specify public access URL, generally not needed for local services, 
  # configure as LAN IP or domain name when service is not on local machine
  public_read_url: "http://localhost:9001"


# MCP Server configuration
server:
  # Timezone setting
  tz: Asia/Shanghai
  
  # MCP Server configuration
  mcp_host: 0.0.0.0
  mcp_port: 9002
  
  # Base service configuration, update here accordingly if you changed server_host or server_port in base section, 
  mcp_base_url: http://localhost:9001
  
  # ComfyUI integration configuration
  # ComfyUI service address
  comfyui_base_url: http://localhost:8188
  # ComfyUI API Key (required if API Nodes are used in workflows, 
  # get it from: https://platform.comfy.org/profile/api-keys)
  comfyui_api_key: ""
  # Cookies used when calling ComfyUI interface, configure this if ComfyUI service requires authentication, supports URL/JSON/string formats
  comfyui_cookies: ""
  # Executor type for calling ComfyUI interface, supports http and websocket (both are generally supported)
  comfyui_executor_type: http


# MCP Client configuration
client:
  # Timezone setting
  tz: Asia/Shanghai
  
  # MCP Client Web service configuration
  # Chainlit framework configuration (used for chainlit auth, can be reused or randomly generated)
  chainlit_auth_secret: "8Z$9@,BzJv*sxlyfj9JVWDIltov5kx%Buc*kA>O>.oLDsEwGuD.Zm~2y3vBk,m_A"
  # Chainlit configuration, see: https://docs.chainlit.io/backend/command-line
  chainlit_host: 127.0.0.1
  chainlit_port: 9003
  chainlit_root_path: ""
  chainlit_auth_enabled: true
  chaintlit_save_starter_enabled: false
  
  # Base service configuration, update here accordingly if you changed server_host or server_port in base section, 
  mcp_base_url: http://localhost:9001
  
  # LLM model configuration (at least one of OpenAI and Ollama must be configured, the configured model needs to support tool calling)
  # Any model compatible with the OpenAI API and supporting tool calls is acceptable
  openai_base_url: https://api.openai.com/v1
  openai_api_key: ""
  # List OpenAI models to be used, if multiple, separate with English commas
  chainlit_chat_openai_models: "gpt-4.1"
  
  # Ollama configuration (local models)
  ollama_base_url: http://localhost:11434/v1
  # List Ollama models to be used, if multiple, separate with English commas
  ollama_models: "qwen3:latest"
  
  # Optional, default model for conversations (must be in chainlit_chat_openai_models or ollama_models above)
  chainlit_chat_default_model: ""
